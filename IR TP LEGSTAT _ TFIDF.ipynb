{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IR TP LEGSTAT : TFIDF.ipynb","provenance":[{"file_id":"1GAlbRq0xb4Vs1E033CxsL4cH5oMT_Zwa","timestamp":1634808263839},{"file_id":"1ogOdTxrww0JisAsKtFwnHS3vOxr_OoNM","timestamp":1634807031274}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AE-Z6ydbo3JZ"},"source":["# Introduction\n","\n","This noteook contains implementation of a TF-IDF retreival for LEGSTAT IR Term Project. \n","\n","There are 197 statutes (documents) and 50 train queries. The task is to train TFIDF model and generate trec file for 10 test queries.\n","\n","## Authors\n","- Sayan Mahapatra\n","- Mainak Chowdhury\n","- Upasana Mandal\n","- Khyati Puhup\n"]},{"cell_type":"markdown","metadata":{"id":"5fCMhS2-UNJV"},"source":["# Setup Environment\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6N21QAQL4-_","executionInfo":{"status":"ok","timestamp":1637565537107,"user_tz":-330,"elapsed":1606,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"5a0ee3e3-11f6-4714-a8d4-6f7b00ccae5f"},"source":["!rm -rf sample_data/\n","!rm -rf IRTP/\n","!git clone https://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@github.com/MeSayan/IRTP.git\n","!cd IRTP/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'IRTP'...\n","remote: Enumerating objects: 249, done.\u001b[K\n","remote: Counting objects: 100% (249/249), done.\u001b[K\n","remote: Compressing objects: 100% (238/238), done.\u001b[K\n","remote: Total 249 (delta 10), reused 247 (delta 8), pack-reused 0\u001b[K\n","Receiving objects: 100% (249/249), 547.48 KiB | 13.04 MiB/s, done.\n","Resolving deltas: 100% (10/10), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zwkeofMpWkA","executionInfo":{"status":"ok","timestamp":1636771981062,"user_tz":-330,"elapsed":15874,"user":{"displayName":"Upasana Mandal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYy9uHPYgFRlRRw1BPP3b7dpU1NlBigc29lvnJGA=s64","userId":"09991678866605333746"}},"outputId":"77a16060-84ed-4e35-c1d5-3f4244c65397"},"source":["!echo -e \" scikit-learn==1.0 \\n numpy==1.19.5 \\n pandas==1.1.5 \\n nltk==3.4\" > requirements.txt\n","!pip install -U -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn==1.0\n","  Downloading scikit_learn-1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n","\u001b[K     |████████████████████████████████| 23.1 MB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n","Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.5)\n","Collecting nltk==3.4\n","  Downloading nltk-3.4.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 44.9 MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4->-r requirements.txt (line 4)) (1.15.0)\n","Collecting singledispatch\n","  Downloading singledispatch-3.7.0-py2.py3-none-any.whl (9.2 kB)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1436396 sha256=634c4fffe3cb342b282be98ab3b6990cd447d7c732587afa6de6a8e947307166\n","  Stored in directory: /root/.cache/pip/wheels/13/b8/81/2349be11dd144dc7b68ab983b58cd2fae353cdc50bbdeb09d0\n","Successfully built nltk\n","Installing collected packages: threadpoolctl, singledispatch, scikit-learn, nltk\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.4 scikit-learn-1.0 singledispatch-3.7.0 threadpoolctl-3.0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"pAinMhjTsOa0"},"source":["# Functions\n","\n","- get_all_documents() // return list of documents \n","- get_all_queries() // return list of queries     \n","- clean() // tokenization, stop word, punctuation removal      \n","- preprocessor() // lemmatization, steming etc    \n","- generate_doc_vectors() // tf_idf vectors        \n","- generate_query_vector() // tf_idf vector of query \n","- evaluate_docs() // compute similarity of doc vector and query vector \n","- generate_trec_file() // generate trek file for evaluatiob by trec tool "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-aX2uaFwIri","executionInfo":{"status":"ok","timestamp":1636771984673,"user_tz":-330,"elapsed":1459,"user":{"displayName":"Upasana Mandal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYy9uHPYgFRlRRw1BPP3b7dpU1NlBigc29lvnJGA=s64","userId":"09991678866605333746"}},"outputId":"0538404a-788a-42b1-c981-286964b062b5"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","import sklearn\n","import numpy as np\n","import string\n","\n","import nltk\n","import os\n","import glob\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","\n","print(sklearn.__version__)\n","print(np.__version__)\n","print(pd.__version__)\n","print(nltk.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["1.0\n","1.19.5\n","1.1.5\n","3.4\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"eftNmuzetIjz"},"source":["def get_all_documents():\n","  path = \"IRTP/Object_statutes/*.txt\"\n","  doc_vex=glob.glob(path)\n","  doc_vex.sort(key=lambda f: int(re.sub('\\D', '', f)))\n","  doc_head=[]\n","  doc_cont=[]\n","  for i in doc_vex:\n","    storex=\"\"\n","    f=open(i,\"r\")\n","    for j in f:\n","      storex+=j #store file content in storex and append the sting in doc_cont \n","    doc_cont.append(storex)\n","    doct=i.split(\"IRTP/Object_statutes/\")\n","    doctx=doct[1].split(\".txt\")\n","    doc_head.append(doctx[0]) #contains the file name (Except .txt)\n","  return doc_head,doc_cont"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjaW24lItf-I"},"source":["def get_all_queries(pathx):\n","  fx=open(pathx,\"r\") \n","  quer_vec_head=[]\n","  quer_vec_cont=[]\n","  for j in fx:\n","    stor=j.split(\"||\")\n","    quer_vec_head.append(stor[0]) #take query names like AILA_Q1,AILA_Q2 etc\n","    quer_vec_cont.append(stor[1]) #take query details of each query AILA_Qi i in 1...n, n is number of queries\n","  return quer_vec_head,quer_vec_cont\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JtfRW9SnNpU"},"source":["def clean(items):\n","  \"\"\" Tokenize string, remove punctuation & stopwords \"\"\"\n","  words = []\n","  cleaned_docs = []\n","  st = set(stopwords.words('english'))\n","  for item in items:\n","    sentences = sent_tokenize(item)\n","    lowercase_words = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n","    \n","    # custom Filtering\n","    # 1. w.e.f.<Date> -> [w.e.f., <Date>]\n","    # 2. w.r.e.f.<Date> -> [w.r.e.f, <Date>]\n","    # 3. X.-Y -> [X, Y]\n","    # 4. X.—Y -> [X, Y]\n","    # 5. X- -> X\n","    # 6. -X -> X\n","    # 7. .X -> X\n","    # 8. X. -> X\n","    # 9. 'X or X' -> X\n","    # 10. X-Y -> [X, Y]\n","    nl = []\n","    for word in lowercase_words:\n","      if 'w.e.f.' in word:\n","        a, b = word.split('w.e.f.', 1)\n","        nl.append(a)\n","        nl.append(b)\n","      elif 'w.r.e.f.' in word:\n","        a, b = word.split('w.r.e.f', 1)\n","        nl.append(a)\n","        nl.append(b)\n","      elif '.-' in word:\n","        nl.extend(word.split('.-'))\n","      elif '.—' in word:\n","        nl.extend(word.split('.—'))\n","      elif (word.endswith('-') and not word.endswith('/-')) or ((word.endswith('—') and not word.endswith('/—'))):\n","        nl.append(word[:-1])\n","      elif word.startswith('-') or word.startswith('—'):\n","        nl.append(word[1:])\n","      elif word.startswith(\".\"):\n","        nl.append(word[1:])\n","      elif word.endswith(\".\"):\n","        nl.append(word[:-1])\n","      elif word.startswith(\"'\") and word.endswith(\"'\"):\n","        nl.append(word[1:-1])\n","      elif word.startswith(\"'\"):\n","        nl.append(word[1:])\n","      elif word.endswith(\"'\"):\n","        nl.append(word[:-1])\n","      elif '-' in word:\n","        nl.extend(word.split('-'))\n","      else:\n","        nl.append(word)\n","\n","    punctuation_symbols = string.punctuation + '‘’“”—``'\n","    punctuation_removed_words = [word for word in nl if not word in punctuation_symbols]\n","    stopwords_removed_words = [word for word in punctuation_removed_words if not word in st]\n","    n2 = [word for word in stopwords_removed_words \n","          if (re.match(r\"^[']?[a-z]*[-]{0,1}[a-z]*$\", word) and \n","          word not in ['title', 'desc'] and # Remove 'title' & 'desc'\n","          len(word) > 3 # remove 1 and 2 letter words\n","          )]\n","    words.append(n2)\n","\n","  for words_of_a_sentence in words:\n","    cleaned_docs.append(words_of_a_sentence)\n","\n","  return cleaned_docs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_N3uIjFKtxDP"},"source":["def preprocessor(items):\n","  items = clean(items)\n","  # items is now tokenized and stop words removed\n","  return items\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3T1p0YfkuZv5"},"source":["def pt(doc):\n","  # Use a pass through function since docs already tokenized and preprocessed\n","  return doc\n","\n","def generate_doc_vectors(docs):\n","  global vocab\n","  doc_vectorizer = TfidfVectorizer(tokenizer=pt, preprocessor=pt, use_idf=True, smooth_idf=True)\n","  doc_vectors = doc_vectorizer.fit_transform(docs)\n","  vocab = doc_vectorizer.get_feature_names_out()\n","  df = pd.DataFrame(doc_vectors.todense(), \n","                    index=range(1, len(docs)+1), \n","                    columns=vocab, dtype=np.float64)\n","  return df, doc_vectorizer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpg2jEbCNHsd"},"source":["def generate_query_vectors(vectorizer, queries):\n","  query_vectors = vectorizer.transform(queries)\n","  df = pd.DataFrame(query_vectors.todense(), index=range(1, len(queries)+1),\n","                   columns=vocab, dtype=np.float64)\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6t9_NERShafm"},"source":["def generate_trec_file(file_name):\n","  with open(file_name, \"w\") as f:\n","    for q in range(len(queries)):\n","      drv = C[q]\n","      sdrv = np.flip(np.argsort(drv), axis = 0)\n","      c = 1\n","      episilon = 0\n","      for d in sdrv:\n","        if C[q][d] > episilon:\n","          print(f\"AILA_Q{q+1} Q0 {doc_head[d]} {c} {C[q][d]} LEG_STAT_TRIER R2\", file=f)\n","          c += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsU0QQV_HkuS"},"source":["## Generate Trec File"]},{"cell_type":"code","metadata":{"id":"X_K7pgn4ioWc"},"source":["doc_head, docs = get_all_documents()\n","query_head, queries = get_all_queries(\"IRTP/Query_doc_test.txt\")\n","docs = preprocessor(docs)\n","queries = preprocessor(queries)\n","df_D, doc_vectorizer = generate_doc_vectors(docs)\n","df_Q = generate_query_vectors(doc_vectorizer, queries)\n","Q = df_Q.to_numpy()\n","D = df_D.to_numpy()\n","C = Q.dot(D.T) # Q * D^T\n","generate_trec_file(\"trec_output_file_test_data.txt\")\n","queries"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YSoV3jTHqKw"},"source":["## Evaluate Trec File (For Training Data)"]},{"cell_type":"code","metadata":{"id":"jMQRIQJDl56Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636772032512,"user_tz":-330,"elapsed":511,"user":{"displayName":"Upasana Mandal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYy9uHPYgFRlRRw1BPP3b7dpU1NlBigc29lvnJGA=s64","userId":"09991678866605333746"}},"outputId":"157a4c03-d6fb-4c6e-8bad-ad5010700777"},"source":["!chmod a+x IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval\n","!IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval  IRTP/relevance_judgements_train.txt ./trec_output_file_test_data.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num_q          \tall\t10\n","num_ret        \tall\t1175\n","num_rel        \tall\t44\n","num_rel_ret    \tall\t21\n","map            \tall\t0.0858\n","gm_ap          \tall\t0.0054\n","R-prec         \tall\t0.0750\n","bpref          \tall\t0.0625\n","recip_rank     \tall\t0.1574\n","ircl_prn.0.00  \tall\t0.1609\n","ircl_prn.0.10  \tall\t0.1609\n","ircl_prn.0.20  \tall\t0.1609\n","ircl_prn.0.30  \tall\t0.1422\n","ircl_prn.0.40  \tall\t0.1409\n","ircl_prn.0.50  \tall\t0.1268\n","ircl_prn.0.60  \tall\t0.0313\n","ircl_prn.0.70  \tall\t0.0312\n","ircl_prn.0.80  \tall\t0.0312\n","ircl_prn.0.90  \tall\t0.0255\n","ircl_prn.1.00  \tall\t0.0255\n","P5             \tall\t0.0600\n","P10            \tall\t0.0400\n","P15            \tall\t0.0400\n","P20            \tall\t0.0400\n","P30            \tall\t0.0300\n","P100           \tall\t0.0170\n","P200           \tall\t0.0105\n","P500           \tall\t0.0042\n","P1000          \tall\t0.0021\n"]}]},{"cell_type":"code","metadata":{"id":"bBeFWFgYNzSJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WqHEzXTEDUlq"},"source":["# References\n","\n","- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","- https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d\n","- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","- http://www.rafaelglater.com/en/post/learn-how-to-use-trec_eval-to-evaluate-your-information-retrieval-system\n","- https://radimrehurek.com/gensim/models/tfidfmodel.html\n","\n","\n"]}]}