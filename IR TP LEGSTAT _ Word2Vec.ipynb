{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IR TP LEGSTAT : Word2Vec.ipynb","provenance":[{"file_id":"1eOYYlQjyigPECndV-GHqXxTrIWXUUvzq","timestamp":1636821285264},{"file_id":"1wpDje7Yx2cvYryjW0SqaCYzppgzvX5M5","timestamp":1636602533666},{"file_id":"1GAlbRq0xb4Vs1E033CxsL4cH5oMT_Zwa","timestamp":1634808263839},{"file_id":"1ogOdTxrww0JisAsKtFwnHS3vOxr_OoNM","timestamp":1634807031274}],"collapsed_sections":["AE-Z6ydbo3JZ"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AE-Z6ydbo3JZ"},"source":["# Introduction\n","\n","This noteook contains implementation of a Word2Vec based retreival for LEGSTAT IR Term Project. \n","\n","There are 197 statutes (documents) and 50 train queries. The task is to generate trec file for 10 test queries.\n","\n","## Authors\n","- Sayan Mahapatra\n","- Mainak Chowdhury\n","- Upasana Mandal\n","- Khyati Puhup\n"]},{"cell_type":"markdown","metadata":{"id":"5fCMhS2-UNJV"},"source":["# Setup Environment\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6N21QAQL4-_","executionInfo":{"status":"ok","timestamp":1637565712026,"user_tz":-330,"elapsed":1012,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"ad2a53c4-a0b4-4760-ea25-f9093a0ccbb7"},"source":["!rm -rf sample_data/\n","!rm -rf IRTP/\n","!git clone https://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@github.com/MeSayan/IRTP.git\n","!cd IRTP/\n","!chmod a+x IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'IRTP'...\n","remote: Enumerating objects: 249, done.\u001b[K\n","remote: Counting objects: 100% (249/249), done.\u001b[K\n","remote: Compressing objects: 100% (238/238), done.\u001b[K\n","remote: Total 249 (delta 10), reused 247 (delta 8), pack-reused 0\u001b[K\n","Receiving objects: 100% (249/249), 547.48 KiB | 5.07 MiB/s, done.\n","Resolving deltas: 100% (10/10), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7zwkeofMpWkA","executionInfo":{"status":"ok","timestamp":1636821451881,"user_tz":-330,"elapsed":10930,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"ec7a7c39-3ad4-440b-c60b-5b4069d7badb"},"source":["!echo -e \" scikit-learn==1.0 \\n numpy==1.19.5 \\n pandas==1.1.5 \\n nltk==3.4 \\n gensim\" > requirements.txt\n","!pip install -U -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn==1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.0)\n","Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n","Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.5)\n","Requirement already satisfied: nltk==3.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.4)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.6.0)\n","Collecting gensim\n","  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[K     |████████████████████████████████| 24.1 MB 2.8 kB/s \n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (3.0.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2018.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: singledispatch in /usr/local/lib/python3.7/dist-packages (from nltk==3.4->-r requirements.txt (line 4)) (3.7.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r requirements.txt (line 5)) (5.2.1)\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-4.1.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"pAinMhjTsOa0"},"source":["# Functions\n","\n","- get_all_documents() // return list of documents \n","- get_all_queries() // return list of queries     \n","- clean() // tokenization, stop word, punctuation removal      \n","- preprocessor() // lemmatization, steming etc    \n","- generate() // return vectors (embeddings) for query / docs \n","- evaluate_docs() // compute similarity of doc vector and query vector \n","- generate_test_trec_file() // generate test trec file \n","- generate_test_trec_file() // generate tain trec file for evaluation by trec tool"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-aX2uaFwIri","executionInfo":{"status":"ok","timestamp":1636821388109,"user_tz":-330,"elapsed":28082,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"968a1dc5-b740-4dd7-98ad-f3008907a267"},"source":["import pandas as pd\n","import sklearn\n","import numpy as np\n","import string\n","import pprint\n","\n","pp = pprint.PrettyPrinter()\n","\n","import torch\n","import logging\n","\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import nltk\n","import os\n","import glob\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","\n","from sklearn.preprocessing import normalize\n","\n","print(sklearn.__version__)\n","print(np.__version__)\n","print(pd.__version__)\n","print(nltk.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["1.0\n","1.19.5\n","1.1.5\n","3.4\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"eftNmuzetIjz"},"source":["def get_all_documents():\n","  path = \"IRTP/Object_statutes/*.txt\"\n","  doc_vex=glob.glob(path)\n","  doc_vex.sort(key=lambda f: int(re.sub('\\D', '', f)))\n","  doc_head=[]\n","  doc_cont=[]\n","  for i in doc_vex:\n","    storex=\"\"\n","    f=open(i,\"r\")\n","    for j in f:\n","      storex+=j #store file content in storex and append the sting in doc_cont \n","    doc_cont.append(storex)\n","    doct=i.split(\"IRTP/Object_statutes/\")\n","    doctx=doct[1].split(\".txt\")\n","    doc_head.append(doctx[0]) #contains the file name (Except .txt)\n","  return doc_head,doc_cont"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjaW24lItf-I"},"source":["def get_all_queries(pathx):\n","  fx=open(pathx,\"r\") \n","  quer_vec_head=[]\n","  quer_vec_cont=[]\n","  for j in fx:\n","    stor=j.split(\"||\")\n","    quer_vec_head.append(stor[0]) #take query names like AILA_Q1,AILA_Q2 etc\n","    quer_vec_cont.append(stor[1]) #take query details of each query AILA_Qi i in 1...n, n is number of queries\n","  return quer_vec_head,quer_vec_cont\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JtfRW9SnNpU"},"source":["def clean(items):\n","  \"\"\" Tokenize string, remove punctuation & stopwords \"\"\"\n","  words = []\n","  cleaned_docs = []\n","  st = set(stopwords.words('english'))\n","  for item in items:\n","    sentences = sent_tokenize(item)\n","    lowercase_words = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n","    \n","    # custom Filtering\n","    # 1. w.e.f.<Date> -> [w.e.f., <Date>]\n","    # 2. w.r.e.f.<Date> -> [w.r.e.f, <Date>]\n","    # 3. X.-Y -> [X, Y]\n","    # 4. X.—Y -> [X, Y]\n","    # 5. X- -> X\n","    # 6. -X -> X\n","    # 7. .X -> X\n","    # 8. X. -> X\n","    # 9. 'X or X' -> X\n","    # 10. X-Y -> [X, Y]\n","    nl = []\n","    for word in lowercase_words:\n","      if 'w.e.f.' in word:\n","        a, b = word.split('w.e.f.', 1)\n","        nl.append(a)\n","        nl.append(b)\n","      elif 'w.r.e.f.' in word:\n","        a, b = word.split('w.r.e.f', 1)\n","        nl.append(a)\n","        nl.append(b)\n","      elif '.-' in word:\n","        nl.extend(word.split('.-'))\n","      elif '.—' in word:\n","        nl.extend(word.split('.—'))\n","      elif (word.endswith('-') and not word.endswith('/-')) or ((word.endswith('—') and not word.endswith('/—'))):\n","        nl.append(word[:-1])\n","      elif word.startswith('-') or word.startswith('—'):\n","        nl.append(word[1:])\n","      elif word.startswith(\".\"):\n","        nl.append(word[1:])\n","      elif word.endswith(\".\"):\n","        nl.append(word[:-1])\n","      elif word.startswith(\"'\") and word.endswith(\"'\"):\n","        nl.append(word[1:-1])\n","      elif word.startswith(\"'\"):\n","        nl.append(word[1:])\n","      elif word.endswith(\"'\"):\n","        nl.append(word[:-1])\n","      elif '-' in word:\n","        nl.extend(word.split('-'))\n","      else:\n","        nl.append(word)\n","\n","    punctuation_symbols = string.punctuation + '‘’“”—``'\n","    punctuation_removed_words = [word for word in nl if not word in punctuation_symbols]\n","    stopwords_removed_words = [word for word in punctuation_removed_words if not word in st]\n","    n2 = [word for word in stopwords_removed_words \n","          if (re.match(r\"^[']?[a-z]*[-]{0,1}[a-z]*$\", word) and \n","          word not in ['title', 'desc'] and # Remove 'title' & 'desc'\n","          len(word) > 3 # remove 1 and 2 letter words\n","          )]\n","    words.append(n2)\n","\n","  for words_of_a_sentence in words:\n","    cleaned_docs.append(words_of_a_sentence)\n","\n","  return cleaned_docs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_N3uIjFKtxDP"},"source":["def preprocessor(items):\n","  items = clean(items)\n","  # items is now tokenized and stop words removed\n","  return items\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xStfcV-0ZWx1"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"p1cKpB2HKLxH"},"source":["from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec, KeyedVectors\n","# Train model on corpora\n","doc_head, docs = get_all_documents()\n","query_head, queries = get_all_queries(\"IRTP/Query_doc_train.txt\")\n","queries = preprocessor(queries)\n","docs = preprocessor(docs)\n","n_dim = 1000\n","train_data = docs\n","\n","model = Word2Vec(sentences=train_data, vector_size=n_dim, window=300, min_count=1, workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jHqb4x0NMGE"},"source":["from sklearn.preprocessing import normalize\n","def generate_vectors(word_vectors, items, n_dim=1000):\n","  D = []\n","  for i in range(len(items)):\n","    item = items[i]\n","    sum_vec = np.zeros((n_dim))\n","    for word in item:\n","      if word in word_vectors:\n","          sum_vec = sum_vec + word_vectors[word]\n","    sent_vec = sum_vec / len(item)\n","    D.append(sent_vec)\n","  D = np.array(D, dtype=np.float64)\n","  D = normalize(D, axis=1, norm='l2')\n","  return D\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6t9_NERShafm"},"source":["def generate_test_trec_file(D, Q, C, queries, file_name, threshold=0):\n","  with open(file_name, \"w\") as f:\n","    for q in range(len(queries)):\n","      drv = C[q]\n","      sdrv = np.flip(np.argsort(drv), axis = 0)\n","      c = 1\n","      for d in sdrv:\n","        if C[q][d] > threshold:\n","          print(f\"AILA_TQ{q+1} Q0 {doc_head[d]} {c} {C[q][d]} LEG_STAT_TRIER R6\", file=f)\n","          c += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hw56_KSWI2Fb"},"source":["def generate_train_trec_file(D, Q, C, queries, file_name, threshold=0):\n","  with open(file_name, \"w\") as f:\n","    for q in range(len(queries)):\n","      drv = C[q]\n","      sdrv = np.flip(np.argsort(drv), axis = 0)\n","      c = 1\n","      for d in sdrv:\n","        if C[q][d] > threshold:\n","          print(f\"AILA_Q{q+1} Q0 {doc_head[d]} {c} {C[q][d]} LEG_STAT_TRIER R6\", file=f)\n","          c += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4iRUygfzJOSV"},"source":["# Generate Trec & Evaluate Trec File (Training)"]},{"cell_type":"code","metadata":{"id":"Ysx2aX5JJRmR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636826598917,"user_tz":-330,"elapsed":1837,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"369d5e8e-fa2a-4101-8f32-8c73fda52b65"},"source":["# Store just the words + their trained embeddings.\n","word_vectors = model.wv\n","word_vectors.save(\"word2vec.wordvectors\")\n","# Load back with memory-mapping = read-only, shared across processes.\n","word_vectors = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n","\n","\n","doc_head, docs = get_all_documents()\n","docs = preprocessor(docs)\n","\n","query_head, queries = get_all_queries(\"IRTP/Query_doc_train.txt\")\n","queries = preprocessor(queries)\n","\n","print(\"Embedding documents\")\n","D_tr = generate_vectors(word_vectors, docs)\n","\n","print(\"Embedding Querries\")\n","Q_tr = generate_vectors(word_vectors, queries)\n","\n","C_tr = Q_tr.dot(D_tr.T) # Q * D^T\n","\n","print(\"Generating Trec File (Train)\")\n","generate_train_trec_file(D_tr, Q_tr, C_tr, queries, \"trec_output_file_train_data.txt\")\n","\n","print(\"Evaluating Trec File\")\n","#Evaluate\n","!IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval  IRTP/relevance_judgements_train.txt ./trec_output_file_train_data.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding documents\n","Embedding Querries\n","Generating Trec File (Train)\n","Evaluating Trec File\n","num_q          \tall\t50\n","num_ret        \tall\t9850\n","num_rel        \tall\t221\n","num_rel_ret    \tall\t217\n","map            \tall\t0.0944\n","gm_ap          \tall\t0.0536\n","R-prec         \tall\t0.0660\n","bpref          \tall\t0.0542\n","recip_rank     \tall\t0.2091\n","ircl_prn.0.00  \tall\t0.2242\n","ircl_prn.0.10  \tall\t0.2242\n","ircl_prn.0.20  \tall\t0.2242\n","ircl_prn.0.30  \tall\t0.1032\n","ircl_prn.0.40  \tall\t0.0997\n","ircl_prn.0.50  \tall\t0.0923\n","ircl_prn.0.60  \tall\t0.0636\n","ircl_prn.0.70  \tall\t0.0498\n","ircl_prn.0.80  \tall\t0.0467\n","ircl_prn.0.90  \tall\t0.0366\n","ircl_prn.1.00  \tall\t0.0366\n","P5             \tall\t0.0640\n","P10            \tall\t0.0560\n","P15            \tall\t0.0427\n","P20            \tall\t0.0340\n","P30            \tall\t0.0307\n","P100           \tall\t0.0258\n","P200           \tall\t0.0217\n","P500           \tall\t0.0087\n","P1000          \tall\t0.0043\n"]}]},{"cell_type":"markdown","metadata":{"id":"IsU0QQV_HkuS"},"source":["## Generate Trec File (Test)"]},{"cell_type":"code","metadata":{"id":"X_K7pgn4ioWc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636826635188,"user_tz":-330,"elapsed":1455,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"d35de43b-18ca-437d-a2fc-79f74a20c425"},"source":["doc_head, docs = get_all_documents()\n","docs = preprocessor(docs)\n","\n","query_head, queries = get_all_queries(\"IRTP/Query_doc_test.txt\")\n","queries = preprocessor(queries)\n","\n","print(\"Embedding documents\")\n","D_te = generate_vectors(word_vectors, docs)\n","\n","print(\"Embedding Querries\")\n","Q_te = generate_vectors(word_vectors, queries)\n","\n","\n","C_te = Q_te.dot(D_tr.T) # Q * D^T\n","\n","\n","print(\"Generating Trec File (Test)\")\n","generate_test_trec_file(D_te, Q_te, C_te, queries, \"trec_output_file_test_data.txt\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding documents\n","Embedding Querries\n","Generating Trec File (Test)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFF-nBbVbxH7","executionInfo":{"status":"ok","timestamp":1636826217427,"user_tz":-330,"elapsed":339,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"0449e159-3a53-46d2-d91b-4ac1fc219c51"},"source":["A = [['A', 'B'], ['C', 'D']]\n","print(A)\n","B = [['E', 'F'], ['G', 'H']]\n","print(B)\n","A.extend(B)\n","A"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['A', 'B'], ['C', 'D']]\n","[['E', 'F'], ['G', 'H']]\n"]},{"output_type":"execute_result","data":{"text/plain":["[['A', 'B'], ['C', 'D'], ['E', 'F'], ['G', 'H']]"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"1RFrgSaFbxA4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7SAgtqQqbw-V"},"source":[""],"execution_count":null,"outputs":[]}]}