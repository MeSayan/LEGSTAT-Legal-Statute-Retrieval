{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF + LEMMATIZATION (WITH PART OF SPEECH TAGGING) + STEMMING",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE-Z6ydbo3JZ"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This noteook contains implementation of a TF-IDF retreival for LEGSTAT IR Term Project. \n",
        "\n",
        "There are 197 statutes (documents) and 50 train queries. The task is to train TFIDF model and generate trec file for 10 test queries.\n",
        "\n",
        "## Authors\n",
        "- Sayan Mahapatra\n",
        "- Mainak Chaudhury\n",
        "- Upasana Mandal\n",
        "- Khyati Puhup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCMhS2-UNJV"
      },
      "source": [
        "# Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j77xd6Bvosm",
        "outputId": "36a832d9-fc38-4652-ffcb-8ea370f7bca8"
      },
      "source": [
        "!rm -rf sample_data/\n",
        "!rm -rf IRTP/\n",
        "!git clone https://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@github.com/MeSayan/IRTP.git\n",
        "!cd IRTP/\n",
        "!chmod a+x IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IRTP'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (238/238), done.\u001b[K\n",
            "remote: Total 249 (delta 10), reused 247 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (249/249), 547.48 KiB | 5.07 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYV9kDb3vxvy",
        "outputId": "0c51a382-9b40-4b1d-ae0b-6c15892fe3fd"
      },
      "source": [
        "!echo -e \" scikit-learn==1.0 \\n numpy==1.19.5 \\n pandas==1.1.5 \\n nltk==3.2.5\" > requirements.txt\n",
        "!pip install -U -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.0\n",
            "  Downloading scikit_learn-1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.1 MB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.2.5->-r requirements.txt (line 4)) (1.15.0)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-1.0 threadpoolctl-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAinMhjTsOa0"
      },
      "source": [
        "# Functions\n",
        "\n",
        "- get_all_documents() // return list of documents \n",
        "- get_all_queries() // return list of queries     \n",
        "- clean() // tokenization, stop word, punctuation removal      \n",
        "- preprocessor() // lemmatization, steming etc    \n",
        "- generate_doc_vectors() // tf_idf vectors        \n",
        "- generate_query_vector() // tf_idf vector of query \n",
        "- evaluate_docs() // compute similarity of doc vector and query vector \n",
        "- generate_trec_file() // generate trek file for evaluatiob by trec tool \n",
        "- lemmatizing()//lemmatize and stem the words of the string\n",
        "- pos_tagger()//part of speech tagging for words to be lemmatized "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vde7ts4AvyBr",
        "outputId": "09768076-7f46-48e2-ff62-384c739b0d75"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer,SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(sklearn.__version__)\n",
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(nltk.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "1.0\n",
            "1.19.5\n",
            "1.1.5\n",
            "3.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SwjV4SpvyfT"
      },
      "source": [
        "def get_all_documents():\n",
        "  path = \"IRTP/Object_statutes/*.txt\"\n",
        "  doc_vex=glob.glob(path)\n",
        "  doc_vex.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
        "  doc_head=[]\n",
        "  doc_cont=[]\n",
        "  for i in doc_vex:\n",
        "    storex=\"\"\n",
        "    f=open(i,\"r\")\n",
        "    count=0\n",
        "    for j in f:\n",
        "      storex+=j #store file content in storex and append the sting in doc_cont \n",
        "    doc_cont.append(storex)\n",
        "    doct=i.split(\"IRTP/Object_statutes/\")\n",
        "    doctx=doct[1].split(\".txt\")\n",
        "    doc_head.append(doctx[0]) #contains the file name (Except .txt)\n",
        "  return doc_head,doc_cont"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77GmzZgBvyRn"
      },
      "source": [
        "def get_all_queries(pathx):\n",
        "  fx=open(pathx,\"r\") \n",
        "  quer_vec_head=[]\n",
        "  quer_vec_cont=[]\n",
        "  for j in fx:\n",
        "    stor=j.split(\"||\")\n",
        "    quer_vec_head.append(stor[0]) #take query names like AILA_Q1,AILA_Q2 etc\n",
        "    quer_vec_cont.append(stor[1]) #take query details of each query AILA_Qi i in 1...n, n is number of queries\n",
        "  return quer_vec_head,quer_vec_cont"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vao42r-L67Go"
      },
      "source": [
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:         \n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtlZ1VV_vz3D"
      },
      "source": [
        "def clean(items):\n",
        "  \"\"\" Tokenize string, remove punctuation & stopwords \"\"\"\n",
        "  words = []\n",
        "  cleaned_docs = []\n",
        "  st = set(stopwords.words('english'))\n",
        "  for item in items:\n",
        "    sentences = sent_tokenize(item)\n",
        "    lowercase_words = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n",
        "    \n",
        "    # custom Filtering\n",
        "    # 1. w.e.f.<Date> -> [w.e.f., <Date>]\n",
        "    # 2. w.r.e.f.<Date> -> [w.r.e.f, <Date>]\n",
        "    # 3. X.-Y -> [X, Y]\n",
        "    # 4. X.—Y -> [X, Y]\n",
        "    # 5. X- -> X\n",
        "    # 6. -X -> X\n",
        "    # 7. .X -> X\n",
        "    # 8. X. -> X\n",
        "    # 9. 'X or X' -> X\n",
        "    # 10. X-Y -> [X, Y]\n",
        "    nl = []\n",
        "    for word in lowercase_words:\n",
        "      if 'w.e.f.' in word:\n",
        "        a, b = word.split('w.e.f.', 1)\n",
        "        nl.append(a)\n",
        "        nl.append(b)\n",
        "      elif 'w.r.e.f.' in word:\n",
        "        a, b = word.split('w.r.e.f', 1)\n",
        "        nl.append(a)\n",
        "        nl.append(b)\n",
        "      elif '.-' in word:\n",
        "        nl.extend(word.split('.-'))\n",
        "      elif '.—' in word:\n",
        "        nl.extend(word.split('.—'))\n",
        "      elif (word.endswith('-') and not word.endswith('/-')) or ((word.endswith('—') and not word.endswith('/—'))):\n",
        "        nl.append(word[:-1])\n",
        "      elif word.startswith('-') or word.startswith('—'):\n",
        "        nl.append(word[1:])\n",
        "      elif word.startswith(\".\"):\n",
        "        nl.append(word[1:])\n",
        "      elif word.endswith(\".\"):\n",
        "        nl.append(word[:-1])\n",
        "      elif word.startswith(\"'\") and word.endswith(\"'\"):\n",
        "        nl.append(word[1:-1])\n",
        "      elif word.startswith(\"'\"):\n",
        "        nl.append(word[1:])\n",
        "      elif word.endswith(\"'\"):\n",
        "        nl.append(word[:-1])\n",
        "      elif '-' in word:\n",
        "        nl.extend(word.split('-'))\n",
        "      else:\n",
        "        nl.append(word)\n",
        "\n",
        "    punctuation_symbols = string.punctuation + '‘’“”—``'\n",
        "    punctuation_removed_words = [word for word in nl if not word in punctuation_symbols]\n",
        "    stopwords_removed_words = [word for word in punctuation_removed_words if not word in st]\n",
        "    n2 = [word for word in stopwords_removed_words \n",
        "          if (re.match(r\"^[']?[a-z]*[-]{0,1}[a-z]*$\", word) and \n",
        "          word not in ['title', 'desc'] and # Remove 'title' & 'desc'\n",
        "          len(word) > 3 # remove 1 and 2 letter words\n",
        "          )]\n",
        "    words.append(n2)\n",
        "\n",
        "  for words_of_a_sentence in words:\n",
        "    cleaned_docs.append(words_of_a_sentence)\n",
        "\n",
        "  return cleaned_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXObBOc3wRt_"
      },
      "source": [
        "def lemmatizing(items):\n",
        "  itemod=[]\n",
        "  ps=PorterStemmer()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for i in items:\n",
        "    lioli=[]\n",
        "    lioli=nltk.pos_tag(i)\n",
        "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])),lioli))\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "       if tag is None:\n",
        "           # if there is no available tag, append the token as is\n",
        "           temp=word\n",
        "       else:       \n",
        "           # else use the tag to lemmatize the token\n",
        "           temp=lemmatizer.lemmatize(word, tag)\n",
        "       temp=ps.stem(temp)\n",
        "       lemmatized_sentence.append(temp)\n",
        "    itemod.append(lemmatized_sentence)\n",
        "  return itemod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlhMOxvMwSPy"
      },
      "source": [
        "def preprocessor(items):\n",
        "  \"\"\" Todo (Lemmatization / Stemming)\"\"\"\n",
        "  items = clean(items)\n",
        "  # items is now tokenized and stop words removed, now apply lemmatization / stemmming here\n",
        "  #stem = stemed(items)\n",
        "  #items=lemmatizing(items)\n",
        "  lemm=lemmatizing(items)\n",
        "  return (lemm)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1P-EuglwSkA"
      },
      "source": [
        "def pt(doc):\n",
        "  # Use a pass through function since docs already tokenized and preprocessed\n",
        "  return doc\n",
        "\n",
        "def generate_doc_vectors(docs):\n",
        "  global vocab\n",
        "  doc_vectorizer = TfidfVectorizer(tokenizer=pt, preprocessor=pt, use_idf=True, smooth_idf=True)\n",
        "  doc_vectors = doc_vectorizer.fit_transform(docs)\n",
        "  vocab = doc_vectorizer.get_feature_names_out()\n",
        "  df = pd.DataFrame(doc_vectors.todense(), \n",
        "                    index=range(1, len(docs)+1), \n",
        "                    columns=vocab, dtype=np.float64)\n",
        "  return df, doc_vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfHbNsujwS0T"
      },
      "source": [
        "def generate_query_vectors(vectorizer, queries):\n",
        "  query_vectors = vectorizer.transform(queries)\n",
        "  df = pd.DataFrame(query_vectors.todense(), index=range(1, len(queries)+1),\n",
        "                   columns=vocab, dtype=np.float64)\n",
        "  return df\n",
        "def generate_trec_file(file_name):\n",
        "  with open(file_name, \"w\") as f:\n",
        "    for q in range(len(queries)):\n",
        "      drv = C[q]\n",
        "      sdrv = np.flip(np.argsort(drv), axis = 0)\n",
        "      c = 1\n",
        "      episilon = 0\n",
        "      for d in sdrv:\n",
        "        if C[q][d] > episilon:\n",
        "          print(f\"AILA_TQ{q+1} Q0 {doc_head[d]} {c} {C[q][d]} LEG_STAT_TRIER R1\", file=f)\n",
        "          c += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsU0QQV_HkuS"
      },
      "source": [
        "## Generate Trec File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nSA4d3uwKcZ"
      },
      "source": [
        "doc_head, docs = get_all_documents()\n",
        "query_head, queries = get_all_queries(\"IRTP/Query_doc_test.txt\")\n",
        "docs = preprocessor(docs)\n",
        "queries = preprocessor(queries)\n",
        "df_D, doc_vectorizer = generate_doc_vectors(docs)\n",
        "df_Q = generate_query_vectors(doc_vectorizer, queries)\n",
        "Q = df_Q.to_numpy()\n",
        "D = df_D.to_numpy()\n",
        "C = Q.dot(D.T) # Q * D^T\n",
        "generate_trec_file(\"trec_output_file_test_data_tls.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YSoV3jTHqKw"
      },
      "source": [
        "## Evaluate Trec File (For Training Data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGdRp6niwKzR",
        "outputId": "0a4e87e0-07b1-4beb-8fee-6ab4cc64d45b"
      },
      "source": [
        "!chmod a+x IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval\n",
        "!IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval  IRTP/relevance_judgements_train.txt ./trec_output_file_train_data_tls.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_q          \tall\t50\n",
            "num_ret        \tall\t9506\n",
            "num_rel        \tall\t221\n",
            "num_rel_ret    \tall\t211\n",
            "map            \tall\t0.1310\n",
            "gm_ap          \tall\t0.0815\n",
            "R-prec         \tall\t0.1193\n",
            "bpref          \tall\t0.0873\n",
            "recip_rank     \tall\t0.2660\n",
            "ircl_prn.0.00  \tall\t0.2802\n",
            "ircl_prn.0.10  \tall\t0.2802\n",
            "ircl_prn.0.20  \tall\t0.2802\n",
            "ircl_prn.0.30  \tall\t0.1768\n",
            "ircl_prn.0.40  \tall\t0.1508\n",
            "ircl_prn.0.50  \tall\t0.1165\n",
            "ircl_prn.0.60  \tall\t0.0847\n",
            "ircl_prn.0.70  \tall\t0.0789\n",
            "ircl_prn.0.80  \tall\t0.0527\n",
            "ircl_prn.0.90  \tall\t0.0468\n",
            "ircl_prn.1.00  \tall\t0.0468\n",
            "P5             \tall\t0.1000\n",
            "P10            \tall\t0.0620\n",
            "P15            \tall\t0.0667\n",
            "P20            \tall\t0.0580\n",
            "P30            \tall\t0.0513\n",
            "P100           \tall\t0.0310\n",
            "P200           \tall\t0.0211\n",
            "P500           \tall\t0.0084\n",
            "P1000          \tall\t0.0042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqHEzXTEDUlq"
      },
      "source": [
        "# References\n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "- https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "- http://www.rafaelglater.com/en/post/learn-how-to-use-trec_eval-to-evaluate-your-information-retrieval-system\n",
        "- https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
        "- https://github.com/royn5618/Medium_Blog_Codes/blob/master/Text_Normalization_ft_POS_Tagger.ipynb\n",
        "- https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}