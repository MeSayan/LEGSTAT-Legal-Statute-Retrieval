{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final IRTP : BM25 .ipynb","provenance":[{"file_id":"1-MU9d4zYUcHJ5pioec3s8DzDG8ZTbtHt","timestamp":1636907210033},{"file_id":"1tYxI6_IwKFIC5zzV1yW-e-GZVGbXrPnG","timestamp":1636814750708},{"file_id":"1wpDje7Yx2cvYryjW0SqaCYzppgzvX5M5","timestamp":1636772078782},{"file_id":"1GAlbRq0xb4Vs1E033CxsL4cH5oMT_Zwa","timestamp":1634808263839},{"file_id":"1ogOdTxrww0JisAsKtFwnHS3vOxr_OoNM","timestamp":1634807031274}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AE-Z6ydbo3JZ"},"source":["# Introduction\n","\n","This noteook contains implementation of a TF-IDF retreival for LEGSTAT IR Term Project. \n","\n","There are 197 statutes (documents) and 50 train queries. The task is to train TFIDF model and generate trec file for 10 test queries.\n","\n","## Authors\n","- Sayan Mahapatra\n","- Mainak Chowdhury\n","- Upasana Mandal\n","- Khyati Puhup\n"]},{"cell_type":"markdown","metadata":{"id":"5fCMhS2-UNJV"},"source":["# Setup Environment\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6N21QAQL4-_","executionInfo":{"status":"ok","timestamp":1637566019131,"user_tz":-330,"elapsed":1933,"user":{"displayName":"Sayan Mahapatra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04753178572018377019"}},"outputId":"599d9263-565e-446e-e097-36a4b1553e68"},"source":["!rm -rf sample_data/\n","!rm -rf IRTP/\n","!git clone https://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@github.com/MeSayan/IRTP.git\n","!cd IRTP/\n","!chmod a+x IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'IRTP'...\n","remote: Enumerating objects: 249, done.\u001b[K\n","remote: Counting objects: 100% (249/249), done.\u001b[K\n","remote: Compressing objects: 100% (238/238), done.\u001b[K\n","remote: Total 249 (delta 10), reused 247 (delta 8), pack-reused 0\u001b[K\n","Receiving objects: 100% (249/249), 547.48 KiB | 14.80 MiB/s, done.\n","Resolving deltas: 100% (10/10), done.\n"]}]},{"cell_type":"code","metadata":{"id":"7zwkeofMpWkA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636907260527,"user_tz":-330,"elapsed":15043,"user":{"displayName":"Upasana Mandal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYy9uHPYgFRlRRw1BPP3b7dpU1NlBigc29lvnJGA=s64","userId":"09991678866605333746"}},"outputId":"4fee0052-0fe0-4bad-b636-f1c4019bed4f"},"source":["!echo -e \" scikit-learn==1.0 \\n numpy==1.19.5 \\n pandas==1.1.5 \\n nltk==3.4\" > requirements.txt\n","!pip install -U -r requirements.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn==1.0\n","  Downloading scikit_learn-1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.1 MB)\n","\u001b[K     |████████████████████████████████| 23.1 MB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n","Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.5)\n","Collecting nltk==3.4\n","  Downloading nltk-3.4.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 35.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0->-r requirements.txt (line 1)) (1.1.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4->-r requirements.txt (line 4)) (1.15.0)\n","Collecting singledispatch\n","  Downloading singledispatch-3.7.0-py2.py3-none-any.whl (9.2 kB)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4-py3-none-any.whl size=1436397 sha256=081c78b60ef423498f5d30963aff737efbd24a959e91bbf7bc1936a3934d4c5c\n","  Stored in directory: /root/.cache/pip/wheels/13/b8/81/2349be11dd144dc7b68ab983b58cd2fae353cdc50bbdeb09d0\n","Successfully built nltk\n","Installing collected packages: threadpoolctl, singledispatch, scikit-learn, nltk\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.4 scikit-learn-1.0 singledispatch-3.7.0 threadpoolctl-3.0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"pAinMhjTsOa0"},"source":["# Functions\n","\n","- get_all_documents() // return list of documents \n","- get_all_queries() // return list of queries     \n","- clean() // tokenization, stop word, punctuation removal      \n","- preprocessor() // lemmatization, steming etc    \n","- generate_doc_vectors() // tf_idf vectors        \n","- generate_query_vector() // tf_idf vector of query \n","- evaluate_docs() // compute similarity of doc vector and query vector \n","- generate_trec_file() // generate trek file for evaluatiob by trec tool "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-aX2uaFwIri","executionInfo":{"status":"ok","timestamp":1636907262068,"user_tz":-330,"elapsed":1548,"user":{"displayName":"Upasana Mandal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYy9uHPYgFRlRRw1BPP3b7dpU1NlBigc29lvnJGA=s64","userId":"09991678866605333746"}},"outputId":"7378bfab-cb1b-492a-9015-6953adb004ab"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","import sklearn\n","import numpy as np\n","import string\n","\n","import nltk\n","import os\n","import glob\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","\n","print(sklearn.__version__)\n","print(np.__version__)\n","print(pd.__version__)\n","print(nltk.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["1.0\n","1.19.5\n","1.1.5\n","3.4\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"eftNmuzetIjz"},"source":["def get_all_documents():\n","  path = \"IRTP/Object_statutes/*.txt\"\n","  doc_vex=glob.glob(path)\n","  doc_vex.sort(key=lambda f: int(re.sub('\\D', '', f)))\n","  doc_head=[]\n","  doc_cont=[]\n","  for i in doc_vex:\n","    storex=\"\"\n","    f=open(i,\"r\")\n","    for j in f:\n","      storex+=j #store file content in storex and append the sting in doc_cont \n","    doc_cont.append(storex)\n","    doct=i.split(\"IRTP/Object_statutes/\")\n","    doctx=doct[1].split(\".txt\")\n","    doc_head.append(doctx[0]) #contains the file name (Except .txt)\n","  return doc_head,doc_cont"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjaW24lItf-I"},"source":["def get_all_queries(pathx):\n","  fx=open(pathx,\"r\") \n","  quer_vec_head=[]\n","  quer_vec_cont=[]\n","  for j in fx:\n","    stor=j.split(\"||\")\n","    quer_vec_head.append(stor[0]) #take query names like AILA_Q1,AILA_Q2 etc\n","    quer_vec_cont.append(stor[1]) #take query details of each query AILA_Qi i in 1...n, n is number of queries\n","  return quer_vec_head,quer_vec_cont\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JtfRW9SnNpU"},"source":["def clean(items):\n","  \"\"\" Tokenize string, remove punctuation & stopwords \"\"\"\n","  words = []\n","  cleaned_docs = []\n","  st = set(stopwords.words('english'))\n","  for item in items:\n","    sentences = sent_tokenize(item)\n","    lowercase_words = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n","    \n","    # custom Filtering\n","    # 1. w.e.f.<Date> -> [w.e.f., <Date>]\n","    # 2. w.r.e.f.<Date> -> [w.r.e.f, <Date>]\n","    # 3. X.-Y -> [X, Y]\n","    # 4. X.—Y -> [X, Y]\n","    # 5. X- -> X\n","    # 6. -X -> X\n","    # 7. .X -> X\n","    # 8. X. -> X\n","    # 9. 'X or X' -> X\n","    # 10. X-Y -> [X, Y]\n","    nl = []\n","    for word in lowercase_words:\n","      if 'w.e.f.' in word:\n","        a, b = word.split('w.e.f.', 1)\n","        nl.append(a)\n","        nl.append(b)\n","      elif 'w.r.e.f.' in word:\n","        a, b = word.split('w.r.e.f', 1)\n","        nl.append(a)\n","        nl.append(b)\n","      elif '.-' in word:\n","        nl.extend(word.split('.-'))\n","      elif '.—' in word:\n","        nl.extend(word.split('.—'))\n","      elif (word.endswith('-') and not word.endswith('/-')) or ((word.endswith('—') and not word.endswith('/—'))):\n","        nl.append(word[:-1])\n","      elif word.startswith('-') or word.startswith('—'):\n","        nl.append(word[1:])\n","      elif word.startswith(\".\"):\n","        nl.append(word[1:])\n","      elif word.endswith(\".\"):\n","        nl.append(word[:-1])\n","      elif word.startswith(\"'\") and word.endswith(\"'\"):\n","        nl.append(word[1:-1])\n","      elif word.startswith(\"'\"):\n","        nl.append(word[1:])\n","      elif word.endswith(\"'\"):\n","        nl.append(word[:-1])\n","      elif '-' in word:\n","        nl.extend(word.split('-'))\n","      else:\n","        nl.append(word)\n","\n","    punctuation_symbols = string.punctuation + '‘’“”—``'\n","    punctuation_removed_words = [word for word in nl if not word in punctuation_symbols]\n","    stopwords_removed_words = [word for word in punctuation_removed_words if not word in st]\n","    n2 = [word for word in stopwords_removed_words \n","          if (re.match(r\"^[']?[a-z]*[-]{0,1}[a-z]*$\", word) and \n","          word not in ['title', 'desc'] and # Remove 'title' & 'desc'\n","          len(word) > 3 # remove 1 and 2 letter words\n","          )]\n","    words.append(n2)\n","\n","  for words_of_a_sentence in words:\n","    cleaned_docs.append(words_of_a_sentence)\n","\n","  return cleaned_docs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_N3uIjFKtxDP"},"source":["def preprocessor(items):\n","  items = clean(items)\n","  # items is now tokenized and stop words removed\n","  return items\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YSoV3jTHqKw"},"source":["## Evaluate Trec File (For Training Data)"]},{"cell_type":"code","metadata":{"id":"bBeFWFgYNzSJ"},"source":["class BM25:\n","    # b and k_1 are hyper parameters for BM25.\n","    # setting default vales for k1 = 1.5 and b = 0.75\n","    #used for initialising values\n","    def __init__(set, k1 = 1.5, b = 0.75):\n","        set.b = b\n","        set.k1 = k1\n","    #Fitting the different variables for calculating BM25\n","    def fit(set, statutes):\n","        N = 0 #number of documents\n","        tf = [] #stores term frequency per document\n","        df = {} #stores document frequency per item\n","        D = [] #stores number of term in document\n","        idf = {} #inverse document frequency\n","        for document in statutes:\n","            N = N + 1\n","            D.append(len(document))\n","            f = {} #stores number of times term q_i occurs in Document D.\n","            for term in document:\n","                t_c = f.get(term, 0) + 1\n","                f[term] = t_c\n","            tf.append(f)\n","            for term, _ in f.items():\n","                df_c = df.get(term, 0) + 1\n","                df[term] = df_c\n","        for term, i in df.items():\n","            idf[term] = np.log(1+(N-i+0.5)/(i+0.5))\n","        set.tf_ = tf\n","        set.df_ = df\n","        set.idf_ = idf\n","        set.D_ = D\n","        set.statutes_ = statutes\n","        set.N_ = N\n","        set.davg_ = sum(D)/N #stores average number of term for document\n","        return set\n","    def search(set, query):\n","        scores = [set._score(query, index) for index in range(set.N_)]\n","        return scores\n","    def _score(set, query, index):\n","        score = 0.0\n","        D = set.D_[index]\n","        f = set.tf_[index]\n","        for term in query:\n","            if term not in f:\n","                continue\n","            i = f[term]\n","            upper = set.idf_[term] * i * (set.k1 + 1)\n","            lower = i + set.k1 * (1 - set.b + set.b * D / set.davg_)\n","            score += (upper / lower) #Calculating the score\n","        return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m96CLxNUxdw7"},"source":["import numpy as np\n","from sklearn import preprocessing\n","def generate_trec_file_sayan(filename, query_head, queries, docs_head, docs):\n","  bm25 = BM25()\n","  bm25.fit(docs)\n","  c = 1\n","  with open(filename, \"w\") as f:\n","    for i,q in zip(query_head,queries):\n","      scores = bm25.search(q)\n","      n = 0\n","      for s in scores:\n","        n += s*s\n","      n = n ** 0.5\n","      for j in range(len(scores)):\n","        scores[j] /= n\n","      scores = list(zip(scores, range(len(docs))))\n","      scores.sort(key=lambda x: x[0], reverse=True)\n","      rnk = 1\n","      # print(scores, sep=\"\\n\")\n","      for x in scores:\n","        s, doc_id = x\n","        if s > 0:\n","          print(f\"{i} Q0 {doc_head[doc_id]} {rnk} {s} LEG_STAT_TRIER R4\", file=f)\n","          rnk += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3hLLKYfE_6W"},"source":["doc_head, docs = get_all_documents()\n","query_head, queries = get_all_queries(\"IRTP/Query_doc_train.txt\")\n","docs = preprocessor(docs)\n","queries = preprocessor(queries)\n","generate_trec_file_sayan(\"trec_output_file_train_data.txt\", query_head, queries, doc_head, docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5IJiEUbEHJk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636907289564,"user_tz":-330,"elapsed":18,"user":{"displayName":"Upasana Mandal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYy9uHPYgFRlRRw1BPP3b7dpU1NlBigc29lvnJGA=s64","userId":"09991678866605333746"}},"outputId":"f27c9319-f488-40dd-fe73-e76bc84bdc13"},"source":["!chmod a+x IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval\n","!IRTP/trec_eval.8.1/trec_eval.8.1/trec_eval  IRTP/relevance_judgements_train.txt ./trec_output_file_train_data.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num_q          \tall\t50\n","num_ret        \tall\t9153\n","num_rel        \tall\t221\n","num_rel_ret    \tall\t206\n","map            \tall\t0.0908\n","gm_ap          \tall\t0.0588\n","R-prec         \tall\t0.0707\n","bpref          \tall\t0.0470\n","recip_rank     \tall\t0.1944\n","ircl_prn.0.00  \tall\t0.2096\n","ircl_prn.0.10  \tall\t0.2096\n","ircl_prn.0.20  \tall\t0.2096\n","ircl_prn.0.30  \tall\t0.1135\n","ircl_prn.0.40  \tall\t0.1035\n","ircl_prn.0.50  \tall\t0.0840\n","ircl_prn.0.60  \tall\t0.0603\n","ircl_prn.0.70  \tall\t0.0566\n","ircl_prn.0.80  \tall\t0.0331\n","ircl_prn.0.90  \tall\t0.0280\n","ircl_prn.1.00  \tall\t0.0280\n","P5             \tall\t0.0760\n","P10            \tall\t0.0620\n","P15            \tall\t0.0493\n","P20            \tall\t0.0450\n","P30            \tall\t0.0420\n","P100           \tall\t0.0250\n","P200           \tall\t0.0206\n","P500           \tall\t0.0082\n","P1000          \tall\t0.0041\n"]}]},{"cell_type":"markdown","metadata":{"id":"BAYehube5M7Q"},"source":["# Generate Trec Test File"]},{"cell_type":"code","metadata":{"id":"0cwsLKvI5P_L"},"source":["doc_head_te, docs_te = get_all_documents()\n","query_head_te, queries_te = get_all_queries(\"IRTP/Query_doc_test.txt\")\n","docs_te = preprocessor(docs_te)\n","queries_te = preprocessor(queries_te)\n","generate_trec_file_sayan(\"trec_output_file_test_data.txt\", query_head_te, queries_te, doc_head_te, docs_te)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WqHEzXTEDUlq"},"source":["# References\n","\n","- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","- https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d\n","- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","- http://www.rafaelglater.com/en/post/learn-how-to-use-trec_eval-to-evaluate-your-information-retrieval-system\n","- https://radimrehurek.com/gensim/models/tfidfmodel.html\n","\n","\n"]}]}